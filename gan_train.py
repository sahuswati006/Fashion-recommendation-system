# -*- coding: utf-8 -*-
"""GAN_Train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e6I03FIvIUKJVN-agyDRJlIxh1yVmPzy
"""

from __future__ import division
from __future__ import print_function
import os
import time
import math
import threading
from io import StringIO
from PIL import Image
from glob import glob
import queue
import tensorflow as tf
import numpy as np
from six.moves import xrange

from ops import *
from utils import *

def conv_out_size_same(size, stride):
  return int(math.ceil(float(size) / float(stride)))

def gen_random(mode, size):
    if mode=='normal01': return np.random.normal(0,1,size=size)
    if mode=='uniform_signed': return np.random.uniform(-1,1,size=size)
    if mode=='uniform_unsigned': return np.random.uniform(0,1,size=size)

def image_manifold_size(num_images):
  manifold_h = int(np.ceil(np.sqrt(num_images)))
  manifold_w = int(np.floor(np.sqrt(num_images)))
  return [manifold_h, manifold_w]

# Commented out IPython magic to ensure Python compatibility.
class DCGAN(object):
  def __init__(self, sess, input_height=128, input_width=128, crop=True,
         batch_size=64, sample_num = 64, output_height=64, output_width=64,
         y_dim=6, z_dim=256, gf_dim=64, c_dim=3, dataset_name='default', max_to_keep=1,
         input_fname_pattern='*.jpg', out_dir=None, data_dir=None, checkpoint_dir=None, sample_dir=None):
    
    self.sess = sess
    self.crop = crop

    self.batch_size = batch_size
    self.sample_num = sample_num

    self.input_height = input_height
    self.input_width = input_width
    self.output_height = output_height
    self.output_width = output_width

    self.y_dim = y_dim
    self.z_dim = z_dim
    self.is_train = tf.placeholder(tf.bool, [])

    # batch normalization : deals with poor initialization helps gradient flow
    with tf.variable_scope('GAN'):
        self.d_bn1 = batch_norm(name='d_bn1')
        self.d_bn2 = batch_norm(name='d_bn2')
        self.d_bn3 = batch_norm(name='d_bn3')
        self.d_bn4 = batch_norm(name='d_bn4')


        self.g_bn0 = batch_norm(name='g_bn0')
        self.g_bn1 = batch_norm(name='g_bn1')
        self.g_bn2 = batch_norm(name='g_bn2')
        self.g_bn3 = batch_norm(name='g_bn3')
        self.g_bn4 = batch_norm(name='g_bn4')
        self.g_bn5 = batch_norm(name='g_bn5')
        self.g_bn6 = batch_norm(name='g_bn6')

        self.dataset_name = dataset_name
        self.input_fname_pattern = input_fname_pattern
        self.checkpoint_dir = checkpoint_dir
        self.data_dir = data_dir
        self.out_dir = out_dir
        self.max_to_keep = max_to_keep

        _,_,_,self.data,_,_=np.load('/content/'+self.dataset_name, allow_pickle=True, encoding='bytes')

        self.c_dim = c_dim

        self.build_model()

  def get_gen(self,_z, _y):
    with tf.variable_scope('GAN') as scope:
      scope.reuse_variables()
      return self.generator(_z,_y)

  def get_dis(self,_z, _y):
    with tf.variable_scope('GAN') as scope:
      scope.reuse_variables()
      return self.discriminator(_z,_y) 

  def build_model(self):
    if self.y_dim:
      self.y= tf.placeholder(tf.float32, [self.batch_size, self.y_dim], name='y')
    else:
      self.y = None

    if self.crop:
      image_dims = [self.output_height, self.output_width, self.c_dim]
    else:
      image_dims = [self.input_height, self.input_width, self.c_dim]

    self.inputs = tf.placeholder(
      tf.float32, [self.batch_size] + image_dims, name='real_images')
    
    self.sample_inputs = tf.placeholder(
      tf.float32, [self.sample_num] + image_dims, name='sample_inputs')

    inputs = self.inputs
    sample_inputs = self.sample_inputs

    self.z = tf.placeholder(
      tf.float32, [None, self.z_dim], name='z')
    self.z_sum = histogram_summary("z", self.z)

 
    self.G = self.generator(self.z, self.y)
    self.D, self.D_logits = self.discriminator(inputs, self.y, reuse=False)

    # Fake Sample data classifying by discriminator
    self.sampler = self.sampler(self.z, self.y)
    self.D_fake_logits, self.D_fake = self.discriminator(self.G, self.y, reuse=True)
  
    self.d_sum = histogram_summary("d", self.D)
    self.d__sum = histogram_summary("d_", self.D_fake)
    self.G_sum = image_summary("G", self.G)

    # def sigmoid_cross_entropy_with_logits(x, y):
    #   try:
    #     return tf.nn.sigmoid_cross_entropy_with_logits(logits=x, labels=y)
    #   except:
    #     return tf.nn.sigmoid_cross_entropy_with_logits(logits=x, targets=y)


    self.d_loss_real = tf.reduce_mean(tf.square(self.D-1))
    self.d_loss_fake = tf.reduce_mean(tf.square(self.D_fake))
    self.g_loss = tf.reduce_mean(tf.square(self.D_fake-1))

    self.d_loss_real_sum = scalar_summary("d_loss_real", self.d_loss_real)
    self.d_loss_fake_sum = scalar_summary("d_loss_fake", self.d_loss_fake)
                          
    self.d_loss = (self.d_loss_real + self.d_loss_fake)/2

    self.g_loss_sum = scalar_summary("g_loss", self.g_loss)
    self.d_loss_sum = scalar_summary("d_loss", self.d_loss)

    t_vars = tf.trainable_variables()

    self.d_vars = [var for var in t_vars if 'd_' in var.name]
    self.g_vars = [var for var in t_vars if 'g_' in var.name]

    self.saver = tf.train.Saver([k for k in tf.global_variables() if k.name.startswith('GAN')])

  def train(self, config):
    d_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1, beta2=0.99).minimize(self.d_loss, var_list=self.d_vars)
    g_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1, beta2=0.99).minimize(self.g_loss, var_list=self.g_vars)

    try:
      tf.global_variables_initializer().run()
    except:
      tf.initialize_all_variables().run()

    self.g_sum = merge_summary([self.z_sum, self.d__sum, self.d_loss_fake_sum, self.g_loss_sum])
    
    self.d_sum = merge_summary(
        [self.z_sum, self.d_sum, self.d_loss_real_sum, self.d_loss_sum])
    
    self.writer = SummaryWriter("./logs", self.sess.graph)

    sample_z = gen_random(mode='normal01', size=(self.sample_num , self.z_dim))
    
    def load_batch_from_queue(q):
      imgs=np.zeros([self.batch_size,64,64,3])
      labels=np.zeros([self.batch_size,self.y_dim])
      itemnum=len(self.data)
      while True:
        for i in range(self.batch_size):
          idx = np.random.randint(itemnum)
          jpg=((np.asarray(Image.open(StringIO(self.data[idx][r'imgs'])).convert('RGB').resize((64,64))))-127.5/127.5)     
          y=self.data[idx]['c']
          imgs[i],labels[i]=jpg,y
        q.put((imgs,labels))
    q=queue.Queue(maxsize=5)
        
    for i in range(1):
      t = threading.Thread(target=load_batch_from_queue,args=[q])
      t.start()
    
    if config.dataset == 'mnist':
      sample_inputs = self.data_X[0:self.sample_num]
      sample_labels = self.data_y[0:self.sample_num]
    else:
      sample_files = q.get()
      sample = [
          get_image(sample_file,
                    input_height=self.input_height,
                    input_width=self.input_width,
                    resize_height=self.output_height,
                    resize_width=self.output_width,
                    crop=self.crop,
                    grayscale=self.grayscale) for sample_file in sample_files]
      sample_inputs, sample_labels = q.get()
      
    counter = 1
    start_time = time.time()
    could_load, checkpoint_counter = self.load(self.checkpoint_dir)
    if could_load:
      counter = checkpoint_counter
      print(" [*] Load SUCCESS")
    else:
      print(" [!] Load failed...")

    for epoch in xrange(config.epoch):

      for idx in xrange(0, int(min(len(self.data), config.train_size) // config.batch_size)):
        if config.dataset == 'mnist':
          batch_images = self.data_X[idx*config.batch_size:(idx+1)*config.batch_size]
          batch_labels = self.data_y[idx*config.batch_size:(idx+1)*config.batch_size]
        else:
          batch_files = self.data[idx*config.batch_size:(idx+1)*config.batch_size]
          batch = [
              get_image(batch_file,
                        input_height=self.input_height,
                        input_width=self.input_width,
                        resize_height=self.output_height,
                        resize_width=self.output_width,
                        crop=self.crop,
                        grayscale=self.grayscale) for batch_file in batch_files]
          batch_images,batch_labels=q.get()
          
        batch_z = gen_random(mode='normal01', size=(config.batch_size, self.z_dim)).astype(np.float32)

        if config.dataset == 'mnist':
          # Update D network
          _, summary_str = self.sess.run([d_optim, self.d_sum],
            feed_dict={ 
              self.inputs: batch_images,
              self.z: batch_z,
              self.y:batch_labels,
            })
          self.writer.add_summary(summary_str, counter)

          # Update G network
          _, summary_str = self.sess.run([g_optim, self.g_sum],
            feed_dict={
              self.z: batch_z, 
              self.y:batch_labels,
            })
          self.writer.add_summary(summary_str, counter)

          # Run g_optim twice to make sure that d_loss does not go to zero (different from paper)
          _, summary_str = self.sess.run([g_optim, self.g_sum],
            feed_dict={ self.z: batch_z, self.y:batch_labels })
          self.writer.add_summary(summary_str, counter)
          
          errD_fake = self.d_loss_fake.eval({
              self.z: batch_z, 
              self.y:batch_labels
          })
          errD_real = self.d_loss_real.eval({
              self.inputs: batch_images,
              self.y:batch_labels
          })
          errG = self.g_loss.eval({
              self.z: batch_z,
              self.y: batch_labels
          })

        else:
          # Update D network
          _, summary_str = self.sess.run([d_optim, self.d_sum],
            feed_dict={ 
              self.inputs: batch_images,
              self.z: batch_z,
              self.y:batch_labels,
              self.is_train: True
            })
          self.writer.add_summary(summary_str, counter)

          # Update G network
          _, summary_str = self.sess.run([g_optim, self.g_sum],
            feed_dict={
              self.z: batch_z, 
              self.y:batch_labels,
              self.is_train: True
            })
          
          self.writer.add_summary(summary_str, counter)

          # Run g_optim twice to make sure that d_loss does not go to zero (different from paper)
          
          errD_fake = self.d_loss_fake.eval({
              self.z: batch_z, 
              self.y:batch_labels,
              self.is_train: False
          })
          errD_real = self.d_loss_real.eval({
              self.inputs: batch_images,
              self.y:batch_labels,
              self.is_train: False
          })
          errG = self.g_loss.eval({
              self.z: batch_z,
              self.y: batch_labels,
              self.is_train: False
          })
        

        print("%8d Epoch:[%2d] [%4d/%4d] time: %4.4f, d_loss: %.8f, g_loss: %.8f" % (counter, epoch, idx, batch_idxs, time.time() - start_time, errD_fake+errD_real, errG))
        
        counter += 1
        if np.mod(counter, 100) == 1:
          if config.dataset == 'mnist':
            samples, d_loss, g_loss = self.sess.run(
              [self.sampler, self.d_loss, self.g_loss],
              feed_dict={
                  self.z: sample_z,
                  self.inputs: sample_inputs,
                  self.y:sample_labels,
                  self.is_train: False
              }
            )
            save_images(samples, image_manifold_size(samples.shape[0]),
                  './{}/train_{:02d}_{:04d}.png'.format(config.sample_dir, epoch, idx))
            print("[Sample] d_loss: %.8f, g_loss: %.8f" % (d_loss, g_loss)) 
          else:
            try:
              samples, d_loss, g_loss = self.sess.run(
                [self.sampler, self.d_loss, self.g_loss],
                feed_dict={
                    self.z: sample_z,
                    self.inputs: sample_inputs,
                    self.y:sample_labels,
                    self.is_train: False
                },
              )
              save_images(samples, image_manifold_size(samples.shape[0]),
                    './{}/train_{:02d}_{:04d}.png'.format(config.sample_dir, epoch, idx))
              print("[Sample] d_loss: %.8f, g_loss: %.8f" % (d_loss, g_loss)) 
            except:
              print("one pic error!...")

        if np.mod(counter, 500) == 2:
          self.save(config.checkpoint_dir, counter)

  def discriminator(self, image, y=None, reuse=False):
    with tf.variable_scope("discriminator") as scope:
        if reuse:
            scope.reuse_variables() 
        if not self.y_dim:
          h0 = lrelu(conv2d(image, self.df_dim, name='d_h0_conv'))
          h1 = lrelu(self.d_bn1(conv2d(h0, self.df_dim*2, name='d_h1_conv')))
          h2 = lrelu(self.d_bn2(conv2d(h1, self.df_dim*4, name='d_h2_conv')))
          h3 = lrelu(self.d_bn3(conv2d(h2, self.df_dim*8, name='d_h3_conv')))
          h4 = linear(tf.reshape(h3, [self.batch_size, -1]), 1, 'd_h4_lin')

          return tf.nn.sigmoid(h4), h4

        else:  
          output_height = 64
          yb = tf.reshape(y, [int(y.get_shape()[0]), 1, 1, self.y_dim])
          x = conv_cond_concat(image, yb)    
          h0 = lrelu(conv2d(x, output_height, name='d_h0_conv'))
          h0 = conv_cond_concat(h0, yb)
          h1 = lrelu(self.d_bn1(conv2d(h0, output_height*2, name='d_h1_conv'),train=self.is_train))
          h1 = conv_cond_concat(h1, yb)
          h2 = lrelu(self.d_bn2(conv2d(h1, output_height*4, name='d_h2_conv'),train=self.is_train))
          h2 = conv_cond_concat(h2, yb)
          h3 = lrelu(self.d_bn3(conv2d(h2, output_height*8, name='d_h3_conv'),train=self.is_train))
          h4=linear(tf.reshape(h3, [int(h3.get_shape()[0]), -1]), 1, 'd_h4_lin')
        
          return tf.nn.sigmoid(h4), h4

  def generator(self, z, y=None):
    with tf.variable_scope("generator") as scope:
        if not self.y_dim:
          s_h, s_w = self.output_height, self.output_width
          s_h2, s_w2 = conv_out_size_same(s_h, 2), conv_out_size_same(s_w, 2)
          s_h4, s_w4 = conv_out_size_same(s_h2, 2), conv_out_size_same(s_w2, 2)
          s_h8, s_w8 = conv_out_size_same(s_h4, 2), conv_out_size_same(s_w4, 2)
          s_h16, s_w16 = conv_out_size_same(s_h8, 2), conv_out_size_same(s_w8, 2)

          # project `z` and reshape
          self.z_, self.h0_w, self.h0_b = linear(
              z, self.gf_dim*8*s_h16*s_w16, 'g_h0_lin', with_w=True)

          self.h0 = tf.reshape(
              self.z_, [-1, s_h16, s_w16, self.gf_dim * 8])
          h0 = tf.nn.relu(self.g_bn0(self.h0))

          self.h1, self.h1_w, self.h1_b = deconv2d(
              h0, [self.batch_size, s_h8, s_w8, self.gf_dim*4], name='g_h1', with_w=True)
          h1 = tf.nn.relu(self.g_bn1(self.h1))

          h2, self.h2_w, self.h2_b = deconv2d(
              h1, [self.batch_size, s_h4, s_w4, self.gf_dim*2], name='g_h2', with_w=True)
          h2 = tf.nn.relu(self.g_bn2(h2))

          h3, self.h3_w, self.h3_b = deconv2d(
              h2, [self.batch_size, s_h2, s_w2, self.gf_dim*1], name='g_h3', with_w=True)
          h3 = tf.nn.relu(self.g_bn3(h3))

          h4, self.h4_w, self.h4_b = deconv2d(
              h3, [self.batch_size, s_h, s_w, self.c_dim], name='g_h4', with_w=True)

          return tf.nn.tanh(h4)

        else:
          z = tf.concat([z, y], 1)
          self.z_, self.h0_w, self.h0_b = linear(z, 4*4*256, 'g_h0_lin', with_w=True)

          self.h0 = tf.reshape(self.z_, [-1, 4, 4, 256])
          h0 = lrelu(self.g_bn0(self.h0,train=self.is_train))

          self.h1, self.h1_w, self.h1_b = deconv2d(h0, [self.batch_size, 8, 8, 256], name='g_h1', with_w=True)
          h1 = lrelu(self.g_bn1(self.h1,train=self.is_train))
          
          self.h2, self.h2_w, self.h2_b = deconv2d(h1, [self.batch_size, 8, 8, 256], stride=1, name='g_h2', with_w=True)
          h2 = lrelu(self.g_bn2(self.h2,train=self.is_train))

          h3, self.h3_w, self.h3_b = deconv2d(h2, [self.batch_size, 16, 16, 256], name='g_h3', with_w=True)
          h3 = lrelu(self.g_bn3(h3,train=self.is_train))

          h4, self.h4_w, self.h4_b = deconv2d(h3, [self.batch_size, 16, 16, 256], stride=1, name='g_h4', with_w=True)
          h4 = lrelu(self.g_bn4(h4,train=self.is_train))

          h5, self.h5_w, self.h5_b = deconv2d(h4, [self.batch_size, 32, 32, 128], name='g_h5', with_w=True)
          h5 = lrelu(self.g_bn5(h5,train=self.is_train))
          
          h6, self.h6_w, self.h6_b = deconv2d(h5, [self.batch_size, 64, 64, 64], name='g_h6', with_w=True)
          h6 = lrelu(self.g_bn6(h6,train=self.is_train))
          
          h7, self.h7_w, self.h7_b = deconv2d(h6, [self.batch_size, 64, 64, 3], stride = 1, name='g_h7', with_w=True)

          return tf.nn.tanh(h7)
    

  def sampler(self, z, y=None):
    with tf.variable_scope("generator") as scope:
        scope.reuse_variables()

        z = tf.concat([z, y], 1)
        self.z_, self.h0_w, self.h0_b = linear(z, 4*4*256, 'g_h0_lin', with_w=True)

        self.h0 = tf.reshape(self.z_, [-1, 4, 4, 256])
        h0 = lrelu(self.g_bn0(self.h0,train=self.is_train))

        self.h1, self.h1_w, self.h1_b = deconv2d(h0, [self.batch_size, 8, 8, 256], name='g_h1', with_w=True)
        h1 = lrelu(self.g_bn1(self.h1,train=self.is_train))
        
        self.h2, self.h2_w, self.h2_b = deconv2d(h1, [self.batch_size, 8, 8, 256], stride=1, name='g_h2', with_w=True)
        h2 = lrelu(self.g_bn2(self.h2,train=self.is_train))

        h3, self.h3_w, self.h3_b = deconv2d(h2, [self.batch_size, 16, 16, 256], name='g_h3', with_w=True)
        h3 = lrelu(self.g_bn3(h3,train=self.is_train))

        h4, self.h4_w, self.h4_b = deconv2d(h3, [self.batch_size, 16, 16, 256], stride=1, name='g_h4', with_w=True)
        h4 = lrelu(self.g_bn4(h4,train=self.is_train))

        h5, self.h5_w, self.h5_b = deconv2d(h4, [self.batch_size, 32, 32, 128], name='g_h5', with_w=True)
        h5 = lrelu(self.g_bn5(h5,train=self.is_train))
        
        h6, self.h6_w, self.h6_b = deconv2d(h5, [self.batch_size, 64, 64, 64], name='g_h6', with_w=True)
        h6 = lrelu(self.g_bn6(h6,train=self.is_train))
        
        h7, self.h7_w, self.h7_b = deconv2d(h6, [self.batch_size, 64, 64, 3], stride=1, name='g_h7', with_w=True)

        return tf.nn.tanh(h7)


  @property
  def model_dir(self):
    return "{}_{}_{}_{}".format(
        self.dataset_name, self.batch_size,
        self.output_height, self.output_width)
      
  def save(self, checkpoint_dir, step, filename="GAN_pretrained_model.model", ckpt=True, frozen=False):
    checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir)

    if not os.path.exists(checkpoint_dir):
      os.makedirs(checkpoint_dir)

    if ckpt:
      self.saver.save(self.sess,
              os.path.join(checkpoint_dir, model_name),
              global_step=step)

  def load(self, checkpoint_dir):
    import re
    print(" [*] Reading checkpoints...", checkpoint_dir)
    checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir)

    ckpt = tf.train.get_checkpoint_state(checkpoint_dir)
    if ckpt and ckpt.model_checkpoint_path:
      ckpt_name = os.path.basename(ckpt.model_checkpoint_path)
      self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name))
      counter = int(next(re.finditer("(\d+)(?!.*\d)",ckpt_name)).group(0))
      #counter = int(ckpt_name.split('-')[-1])
      print(" [*] Success to read {}".format(ckpt_name))
      return True, counter
    else:
      print(" [*] Failed to find a checkpoint")
      return False, 0